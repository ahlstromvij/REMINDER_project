---
# This template is largely functionally equivalent to that for elsevier
# The two fields that are not in the elsevier format are:
#      `corresponding_author` and `acknowledgements`
fontsize: 12pt
title: |
  Is general political knowledge indicative of issue-specific knowledge? 
  Stress-testing the generalist assumption
author:
  - name: William L. Allen
    email: william.allen@politics.ox.ac.uk
    corresponding_author: yes # Footnote using corresponding_author
  - name: Kristoffer Ahlstrom-Vij
    email: k.ahlstrom-vij@bbk.ac.uk
footnote:
  - code: 1
    text: "Current email address: \\href{mailto:cat@example.com}{cat@example.com}."
abstract: |
 \singlespacing Using general political knowledge questions to explain variation in policy-specific outcomes invokes a potentially strong generalist assumption: possessing general knowledge is diagnostic of holding unmeasured, issue-specific knowledge. Is this warranted? Using convergent evidence drawn from best-available U.S. and European surveys, we argue it is. First, factor analysis applied to a cross-national 2018 European data set that, unusually, includes questions about general politics and the specific issue of EU immigration reveals how the questions produce a plausibly unidimensional set. Second, knowledge scales built using Item Response Theory (IRT) modeling of responses to each set of questions display similar associations with known correlates of political knowledge -- gender, age, and education -- that are mirrored in established ANES, CES, and BES general knowledge measures. Finally, these patterns remain after extending our analysis to surveys that test issue-specific knowledge about COVID-19 and climate change. Finally, we offer practical steps for researchers to follow when conducting similar stress-tests on unidimensionality assumptions, and provide the code underlying the present investigation in a public repository as a template for others to use, in line with Open Science principles.

acknowledgements: |
  WA acknowledges support from the British Academy (grant number PF21\210066).
keywords:
  - political knowledge
  - immigration
  - public health
  - climate change
#fontsize: 12pt
spacing: oneline # could also be oneline
#classoptions:
#  - endnotes
bibliography: mybibfile.bib
#csl: mycitationstyle.csl
#link-citations: yes
output: rticles::oup_article
#urlcolor: orange
#linkcolor: green
#citecolor: red
#header-includes:
#  - \usepackage[nomarkers,tablesfirst]{endfloat} # For figures and tables at end
#  - \usepackage{lineno} # For line numbering
#  - \linenumbers # For line numbering
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE) # By default, hide code; set to TRUE to see code
knitr::opts_chunk$set(fig.pos = '!h') # Places figures in text
knitr::opts_chunk$set(out.width = '100%', dpi=300) # Figure resolution and size
knitr::opts_chunk$set(fig.env="figure") # Latex figure environment

library(tidyverse)
library(kableExtra)
library(psych)
library(lavaan)
library(texreg)
library(emmeans)
library(ggpubr)

# global plotting values
global_font_size = 14
# color blind friendly palette
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# read in all data
reminder_data <- read_csv("../../data/model_data_IRT.csv")
bes_data <- read_csv("../../data/bes_data.csv")
anes_data <- read_csv("../../data/anes_data.csv")
cces_data <- read_csv("../../data/cces_data.csv")
covid_data <- read_csv("../../data/covid_data.csv")
epcc_data <- read_csv("../../data/epcc_data.csv")

# total sample size
tot_n <- dim(reminder_data)[1] +
  dim(bes_data)[1] +
  dim(anes_data)[1] +
  dim(cces_data)[1] +
  dim(covid_data)[1] +
  dim(epcc_data)[1]
```

# Introduction

Political knowledge has been called a "workhorse variable" [@kleinberg2022] owing to its usefulness as a proxy for other concepts, including political sophistication and competence [@carpinikeeter1996]. However, survey-based practice has operationalized knowledge in different ways, which potentially threatens conclusions about both its determinants and consequences. In their overview of 335 knowledge questions fielded between 2007-10, @barabas2014 identify two dimensions that characterize these questions: a temporal dimension capturing how recently the factual information emerged; and a topical dimension capturing whether the factual information refers to general institutions or actors, or specific public policy issues. In particular, they focus on the *temporal* dimension of their typology to examine the mechanisms by which voters acquire knowledge.

By contrast, our interest lies primarily in the *topical* dimension of political knowledge questions -- specifically in assessing how general and policy-specific knowledge relate, and what this implies for researchers' necessarily constrained efforts to effectively and efficiently measure voters' levels of knowledge by way of survey instruments. Such an assessment matters because when researchers try to explain variation in *policy-specific* attitudes and behaviors using *general* knowledge items as a way to distinguish between more- and less-informed respondents, they are forced to make a potentially strong assumption: possessing general political knowledge is indicative of holding issue-specific knowledge (and *vice versa*). We call this the *generalist assumption*.

The idea that people who are knowledgeable in one area of politics will be knowledgeable in others has underpinned much of the key work in public opinion [@zaller1986; @zaller1992; @carpinikeeter1996]. Methodologically, this is attractive because it implies that researchers can use a general knowledge scale to measure the extent to which people are politically informed across specific domains, even in the absence of items directly concerned with the particular topics represented within those domains. Yet despite its methodological and practical appeal, this assumption has remained largely untested, partly due to the lack of available survey instruments which contain both general and issue-specific factual items.

In response, we present convergent evidence that stress-tests the generalist assumption by applying a variety of methods to a range of the best-available survey evidence spanning geographic contexts (the U.S. and seven European countries), topics (immigration, public health, and climate change), and respondent panel type (established omnibus and bespoke online panels). Among other steps, we exploit cross-national European survey data that, unusually, contains questions about both general politics and a specific issue, namely European Union immigration [@meltzer2020]. By assessing the dimensionality and construct validity of the full range of knowledge questions contained within our assembled surveys, and by comparing these features to widely-used instruments -- the American National Elections Studies, the Cooperative Election Study, and the British Election Study -- we conclude that the generalist assumption holds up to scrutiny. 

Besides contributing novel empirical evidence for this methodological practice, we conclude by offering practical advice for researchers who either want to conduct similar stress-tests of their own topic- and context-specific survey data on grounds of unidimensionality assumptions, or are considering which types of knowledge questions to include in future studies while acknowledging concerns about resources, time, and survey attentiveness. To that end, and in line with Open Science principles, we also provide the code underlying the present investigation in a public GitHub repository, as a template for others to use.

# The Generalist Assumption about Political Knowledge

Our main contention in this section is that the generalist assumption has come to carry an increasingly heavy burden in political scientific scholarship on knowledge. This is understandable, given that making such an assumption widens the data sets that researchers can use, and thereby drives empirical progress. However, since the most recent, systematic investigation into whether the assumption actually holds was conducted almost three decades ago [@carpinikeeter1993], before the corresponding scholarship relying on that assumption had expanded with regards to both content and geography, another investigation is overdue. In this section, we elaborate on this narrative while clarifying the distinction between general and issue-specific knowledge.

## Generalist versus Specific Knowledge: Conceptual Issues {#conceptual-issues}

The generalist assumption can be traced back at least to John Zaller's analysis of information items in the 1985 National Election Study (NES) Pilot, where he argued that "political information is a relatively general trait that can be effectively measured with a general-purpose information scale" [@zaller1986, 2]. This conclusion subsequently informed his landmark study on public opinion, where he declared that he was "assuming that persons who are knowledgeable about politics in general are habitually attentive to communications on most particular issues as well" [@zaller1992, 43]. @carpinikeeter1993 investigated and defended that assumption, which later featured in both @bartels1996 and @althaus2003's highly influential work "information effects," i.e. the extent to which voters would express different preferences had they possessed more information.

What, then, does it mean to be knowledgeable about politics in general, as Zaller puts it? It is helpful to follow @carpinikeeter1996 in conceptualizing political knowledge as a *resource*: having a lot of it means being in a position to navigate the political world, and to stand a better chance of connecting your fundamental political goals with successful means -- in the simplest case, by voting for the candidate or party that is best place to realize those goals (see also @zaller1992 on how the politically informed tend to be the most ideologically coherent). It is against this background that Delli Carpini and Keeter identify (general) political knowledge as consisting in

> [...] a general familiarity with (1) the rules of the game (the institutions and processes of elections and governance); (2) the substance of politics (the major domestic and international issues of the day, current social and economic conditions, key political initiatives, and so forth); and (3) people and parties (the promises, performances, and attributes of candidates, public officials, and the political parties) [@carpinikeeter1996, 14]

Knowledge of these areas is "critical to the maintenance of a healthy democracy" [@carpinikeeter1996, 14]. Specifically, a person with such knowledge inhabits the position of a (more) effective political actor, in the minimal case as a voter.[^1] That said, considering @carpinikeeter1993's recommended five-item index (see also @carpinikeeter1996, 304-5), the emphasis is very much on the the rules of the game and the main political actors -- on "what the government is and does" as they put it. In their index, the component items ask (1) which party controls the House of Representatives; (2) the percent of the vote required in Congress to override a presidential veto; (3) where the two U.S. parties are located on the political spectrum; (4) which branch of government determines the constitutionality of laws; and (5) who is the U.S. Vice President. 

[^1]: There are important debates about the limitations of this conception of political knowledge. These include observations of how voters often use their lived experiences alongside facts to interpret and engage with political processes [@cramer2017]; and how the formulation of knowledge questions themselves may disadvantage racial and ethnic minorities [@abrajano2015] and women [@kraft2022] by prioritizing some areas of knowledge over others. Two things should be noted here. First, the notion of what we might think of as *technical* political knowledge at work here does not rule out other types of *situated* forms of knowledge -- e.g., knowing what it is like to go hungry for days on end, or to be on the receiving end of racism -- that might enable political agents, not simply to pursue political ends effectively, but to pick morally appropriate ends to begin with. Second, if the type of knowledge necessary for effective political action disadvantages the marginalized, the response should not be to reject such knowledge, but to expand its reach, and thereby empower a large section of society to take effective political action.

As we will explain later (section \ref{construct-validation}), this conception of general political knowledge as being primarily about knowing how government works and what the main political actors are coheres well with the content of knowledge items we tend to see in established data sets such as the American National Elections Studies (ANES), the Cooperative Election Study (CES), and the British Election Study (BES). In the case of the ANES, this is no surprise, since Delli Carpini and Keeter's work fed directly into the knowledge items in that survey, which in turn set the standard for other data sets within and outside of the U.S. By contrast, the type of items that we will consider in relation to immigration, public health (COVID-19), and climate change might get to important issues that people care deeply about, but do not pertain directly to the central resource that prior literature conventionally calls general political knowledge.

A cynic might read this separating out of the "substance of politics" from general knowledge proper as a *post hoc* attempt to categorize established knowledge items and scales as pertaining to general political knowledge. However, we argue there is independent reason for such a separation, given the asymmetry between general and issue-specific knowledge when it comes to putting knowledge into action. While being a generally effective political actor who knows how the system works and who the main players are is necessary (but perhaps not sufficient) for making progress on specific, substantive policy issues, the reverse does not hold. That is, while it might be beneficial to be highly informed on specific issues in order to be an well-motivated and effective political actor, as the literature on issue publics suggests [@kim2009; @henderson2014], it is not necessary.

Of course, some items will not fall uniquely on one side of the distinction between the general and issue-specific. A good example can be found in Wave 17 of the BES [@fieldhouse2018]: "The Liberal Democrats favour a system of proportional representation". This item gets to *both* specific policy positions (and as such to the "substance of politics") and also to the "rules of the game" in relation to the electoral mechanisms of government. What this illustrates is that an item being issue-specific does not *necessarily* take away from it (also) being general (and *vice versa*), and as such being suitable for a general political knowledge scale. Consequently, what matters for the appropriateness of an item in a general knowledge scale is whether it pertains to general political matters -- i.e., to the rules of the game and the main players -- not whether it is *purely* general.

## The Increased Weight Put on the Generalist Assumption

Turning from conceptual matters to the question of measurement, distinguishing between more- and less-informed respondents is a central, practical problem to research that uses knowledge items. On the one hand, this decision could (in line with the previous section) be based on scales that measure respondents' levels of correctness on question batteries, as @carpinikeeter1996 do. On the other hand, it could be inferred from interviewer ratings of respondents' "general level of information about politics and public affairs" [@bartels1996, 203]. Subsequent work across a variety of geographical contexts including the U.S. [@ahlstromvij-posttruth], the U.K. [@ahlstromvij2020-moddemo], Denmark [@bhatti2010; @hansen2009], Sweden [@oscarsson2007] and Canada [@blaisetal2009] follows the norms set by these early studies by using either a variety of general knowledge items or a scale of both general and issue-specific knowledge items alongside interviewer ratings.

As such, the generalist assumption has come to carry an increasingly substantial weight: not only is the catalog of geographical contexts in which researchers investigate the impacts of knowledge on political behaviors expanding, but so also is the list of the specific policy issues that feature in models of voters' attitudes and preferences. Yet, as @barabas2014 identify in their assessment of 355 knowledge items fielded between 2007-10, survey researchers have tended to use questions that test respondents' familiarity with either relatively unchanging aspects of government institutions (107 items comprising 30% of their sample) or details of policy-specific current affairs (113 items comprising 32% of their sample). Consequently, researchers using responses to knowledge items as secondary data, or considering fielding their own items as part of original data collection efforts, will have to use a necessarily limited set of questions -- likely varying along the general-specific dimension -- as a proxy for what respondents know about some other domain that is *actually* of interest.

In this context, it is concerning that the most recent, systematic investigation into the generalist assumption is close to three decades old. In their landmark study of political knowledge in the U.S., @carpinikeeter1996 put the assumption to the test in five surveys: a 1989 Survey of Political Knowledge (N = 610), the 1985 NES Pilot Survey (N = 345), the 1991 NES Pilot Survey (sample size not reported), and two surveys in Virginia, one of which is restricted to the city of Richmond (sample sizes not reported). In regards to method, Delli Carpini and Keeter use confirmatory factor analysis (CFA) to investigate the relative level of fit for uni- versus multidimensional models, followed by construct validation using regression modeling, to investigate whether the scales co-vary in consistent ways with demographic predictors. 

Our analysis goes beyond that of @carpinikeeter1996 in several respects. First, we take advantage of recent methodological developments in scale construction using Item Response Theory (IRT). Second, we rely on significantly more recent data with greater geographical variety. While Delli Carpini and Keeter use exclusively U.S. data (and in two of the surveys, data restricted to a particular U.S. state and city), our data encompasses Germany, Hungary, Poland, Romania, Spain, Sweden, the United Kingdom, and the U.S. Third, our data contain much higher sample sizes. The highest sample size reported by Delli Carpini and Keeter is 610 observations. In our case, the total, combined sample size across our data sets is `r tot_n` observations.

Like Delli Carpini and Keeter, however, our aim is to address whether what we have called the generalist assumption holds. If it does, we expect that different methods of analysis applied to a range of survey data and knowledge questions will converge to support three core expectations: (1) relevant patterns of responses on observed knowledge variables can be plausibly represented by a single latent trait (factor); (2) the relevant knowledge scales (both general and issue-specific) should exhibit similar associations with demographic variables found by prior work to correlate with political knowledge, and in that regard also resemble knowledge scales derived from established political attitudes surveys; and (3) those associations should coincide with equivalent analyses applied to knowledge about other, issue-specific matters.

# Methods and Data

## Dataset

For our main analysis, we use a subset of online survey data collected as part of the REMINDER (Role of European Mobility and Its Impacts in Narratives, Debates and EU Reforms) project [@meltzer2020].[^2] Unlike many existing data sets, this survey has two features which make it uniquely valuable for our purposes. First, it contains both general political knowledge questions *and* issue-specific questions -- in this case, on migration -- in the same survey instrument. This allows us to assess the extent to which these items are collectively unidimensional, as well as whether both sets have similar demographic correlates. Second, it is cross-national, covering seven European countries (Germany, Hungary, Poland, Romania, Spain, Sweden, and the United Kingdom). This enables us to examine whether any results on unidimensionality are artifacts of particular national profiles, e.g. the United States, on which early studies into the unidimensionality of political knowledge were based.

[^2]: Full details about the data set, including its documentation and questionnaire design, are available at <https://doi.org/10.11587/LBSMPQ>.

The REMINDER study used a panel design comprising three waves across 2017-18, whose sampling procedures used quotas by age, gender, and region (at NUTS2 level) to approximate representativeness for each country's adult population. We use data from the second wave (collected between June 6 and July 16, 2018) because our items of interest were only asked in that wave alongside the two sets of knowledge questions. Our sample covers 10,749 respondents, with a breakdown of demographic details appearing in Table \ref{tab:tab1}.

```{r tab1, echo=FALSE, results="asis"}
# create table 1
tbl1 <- reminder_data %>% 
  group_by(nationality) %>% 
  mutate(nationality = str_to_sentence(nationality),
         nationality = recode(nationality,
                              "Uk" = "UK")) %>% 
  rename(Nationality = nationality) %>% 
  summarise("Mean age" = round(mean(age, na.rm=T),0),
            Male = sum(gender=="male"),
            Female = sum(gender=="female"),
            "ISCED 0" = sum(education_ISCED==0),
            "ISCED 1" = sum(education_ISCED==1),
            "ISCED 2" = sum(education_ISCED==2),
            "ISCED 3" = sum(education_ISCED==3),
            "ISCED 4" = sum(education_ISCED==4),
            "ISCED 5" = sum(education_ISCED==5),
            "ISCED 6" = sum(education_ISCED==6),
            "ISCED 7" = sum(education_ISCED==7),
            "ISCED 8" = sum(education_ISCED==8),
            N = n()
  )

kable(t(tbl1), "latex", booktabs = T, caption="Demographic details of the sample", label="tab1") %>%
  kable_styling(latex_options = "scale_down") %>% 
  row_spec(1, bold = T, hline_after = T) %>% 
  row_spec(13, hline_after = T)
```

## General and Specific Knowledge

To measure general political knowledge, we identify correct responses to the following statements:

> (G1) "Switzerland is a member of the EU" (False);\
> (G2) "Every country in the EU elects the same number of representatives to the European Parliament" (False); and\
> (G3) "[NAME OF THE HEAD OF GOVERNMENT] belongs to the [NAME OF CORRECT PARTY]", depending on the survey country.

Then, to measure migration-specific knowledge, we identify correct responses to the following statements:

> (I1) "The free movement of persons is a fundamental right guaranteed by the EU to its citizens" (True);\
> (I2) "Greece is part of the Schengen Area" (True);\
> (I3) "In 2015, Afghans have been the largest group of people that applied for asylum in the EU" (False); and\
> (I4) "In 2015, asylum in the EU was more frequently granted to Syrians than any other nationality" (True).

## Methods {#methods}

To evaluate the core elements of the generalist assumption, we apply a three-step approach that expands on that taken by @carpinikeeter1996 in their aforementioned, early work on knowledge scales. 

First, we use a combination of exploratory (EFA) and confirmatory (CFA) factor analysis to investigate the dimensionality of the political knowledge response data. While Delli Carpini and Keeter rely exclusively on CFA, we find it helpful to start with parallel analysis and EFA in the first step, in order to get an initial sense of likely dimensionality.

Second, we use regression analysis to investigate whether any latent traits exhibit correlations with demographic traits that would be expected if they corresponded to political knowledge (a measure of construct validity). To do this, we go beyond @carpinikeeter1996 in using Item Response Theory (IRT) to both create and validate general and immigration-specific knowledge scales based on the batteries of questions available in the data set [@deayala2009; @demars2010]. We then calculate the estimated marginal mean levels of general political knowledge across levels of education, gender, and age as contained in three reference data sets -- the 2019 American National Elections Studies (ANES) pilot [@anes2019], the 2020 Cooperative Election Study (CES) [@ansolabehere2021], and the 2017 British Election Study (BES) Post-Election Face-to-Face Survey [@fieldhouse2018] -- to investigate whether these exhibit similar patterns as those found in the REMINDER data. Crucially for our stress-testing objective, the general knowledge questions within these omnibus surveys vary along the temporal dimension [@barabas2014]: some tap into respondents' knowledge of established aspects of government and participation, while others measure respondents' surveillance knowledge of politics such as familiarity with current political office-holders.

Third, we replicate this analysis using scales independently derived from surveys that contain knowledge items about other issue-specific matters, namely public health (specifically about COVID-19) and climate change. The COVID-19 questions were part of a bespoke survey experiment fielded in the U.K. (N = 2,917), while the climate change questions came from the U.K. segment of a survey fielded by @pidgeon2016 (N = 1,033). This step addresses concerns that our results are merely artifacts of migration knowledge somehow being uniquely related to general political knowledge.

# Empirical Analysis

## Step 1: Exploratory and Confirmatory Factor Analysis

First, we combine all six items from both knowledge batteries from the REMINDER study, and conduct a parallel analysis to estimate how many factors are likely involved. This suggests four factors in our case. We then apply exploratory factor analysis (EFA) set to four factors. As seen in Table \ref{tab:tab2}, three of the immigration items (I1, I2, and I4) load well onto one factor (Factor 2) while two of the general items (G1 and G2) load well onto another (Factor 4). Further parallel analysis of the three general items (G1-3) suggests one factor, and parallel analysis for the three immigration items does the same, if excluding I3. This offers initial evidence of two separate factors, corresponding to general knowledge and immigration knowledge, respectively.[^3]

[^3]: We do not look at scale reliability as measured by Chronbach's alpha, since such reliability can be high even given multidimensionality [@fabrigar2011].

```{r tab2, echo=FALSE, results="asis"}
fa_tibble <- tibble(
  Item = c("G1","G2","G3","I1","I2","I3","I4"),
  "Factor 1" = c(0.121,0.130,"","","",1.002,0.385),
  "Factor 2" = c(0.139,0.207,"",0.708,0.632,"",0.378),
  "Factor 3" = c(0.210,"",1.000,"","","",0.161),
  "Factor 4" = c(0.529,0.456,"","","","",-0.237)
  )

kable(fa_tibble, 
      "latex", 
      booktabs = T, 
      caption="EFA of general (G) and immigration (I) knowledge questions", 
      label="tab2")
```

Next, we probe further by using confirmatory factor analysis (CFA) to compare the fit of four models against each other: two unidimensional ones and two two-dimensional ones, with the first one in each pair containing all six items and the second one excluding one of these (G3, i.e. the one that failed to load well onto Factor 4 in the earlier EFA). Table \ref{tab:tab3} shows that the model with the best fit is two-dimensional and contains five items, but the unidimensional models also show very good fit. For context, Root Mean Square Error of Approximation (RMSEA) values of less than 0.05 and 0.01 usually are taken to indicate good and very good fit, respectively [@andrews2021]. A Comparative Fit Index (CFI) or Tucker-Lewis Index (TLI) greater than 0.95 [@dima2018], and an Adjusted Goodness-of-fit Index (AFGI) greater than 0.9, are typically taken to suggest good fit [@baumgartner1996].

```{r tab3, echo=FALSE, results="asis"}
mod_1f_6_items <- 'knowledge =~ gen_know_ep + gen_know_party + gen_know_switzerland + mig_know_free_move + mig_know_schengen + mig_know_syrians'
mod_1f_6_items.fit <- cfa(mod_1f_6_items, data=reminder_data, ordered = TRUE)

fit_measures <- data.frame("Measure" = c("rmsea","cfi","tli","agfi"),
                           "1f_6_items" = NA,
                           "1f_5_items" = NA,
                           "2f_6_items" = NA,
                           "2f_5_items" = NA)

fit_measures$X1f_6_items <- c(fitmeasures(mod_1f_6_items.fit)["rmsea"], 
                              fitmeasures(mod_1f_6_items.fit)["cfi"], 
                              fitmeasures(mod_1f_6_items.fit)["tli"], 
                              fitmeasures(mod_1f_6_items.fit)["agfi"])
fit_measures$X1f_6_items <- round(fit_measures$X1f_6_items,3)

mod_1f_5_items <- 'knowledge =~ gen_know_ep + gen_know_switzerland + mig_know_free_move + mig_know_schengen + mig_know_syrians'
mod_1f_5_items.fit <- cfa(mod_1f_5_items, data=reminder_data, ordered = TRUE)

fit_measures$X1f_5_items <- c(fitmeasures(mod_1f_5_items.fit)["rmsea"], 
                              fitmeasures(mod_1f_5_items.fit)["cfi"], 
                              fitmeasures(mod_1f_5_items.fit)["tli"], 
                              fitmeasures(mod_1f_5_items.fit)["agfi"])
fit_measures$X1f_5_items <- round(fit_measures$X1f_5_items,3)

mod_2f_6_items <- '
gen_knowledge =~ gen_know_ep + gen_know_party + gen_know_switzerland
mig_knowledge =~ mig_know_free_move + mig_know_schengen + mig_know_syrians'
mod_2f_6_items.fit <- cfa(mod_2f_6_items, data=reminder_data, ordered = TRUE)

fit_measures$X2f_6_items <- c(fitmeasures(mod_2f_6_items.fit)["rmsea"], 
                              fitmeasures(mod_2f_6_items.fit)["cfi"], 
                              fitmeasures(mod_2f_6_items.fit)["tli"], 
                              fitmeasures(mod_2f_6_items.fit)["agfi"])
fit_measures$X2f_6_items <- round(fit_measures$X2f_6_items,3)

mod_2f_5_items <- '
gen_knowledge =~ gen_know_ep + gen_know_switzerland
mig_knowledge =~ mig_know_free_move + mig_know_schengen + mig_know_syrians'
mod_2f_5_items.fit <- cfa(mod_2f_5_items, data=reminder_data, ordered = TRUE)

fit_measures$X2f_5_items <- c(fitmeasures(mod_2f_5_items.fit)["rmsea"], 
                              fitmeasures(mod_2f_5_items.fit)["cfi"], 
                              fitmeasures(mod_2f_5_items.fit)["tli"], 
                              fitmeasures(mod_2f_5_items.fit)["agfi"])
fit_measures$X2f_5_items <- round(fit_measures$X2f_5_items,3)


fit_measures$Measure <- c("RMSEA",
                          "CFI",
                          "TLI",
                          "AGFI")

fit_measures <- fit_measures %>% 
  rename("1D, 6 items" = X1f_6_items,
         "1D, 5 items" = X1f_5_items,
         "2D, 6 items" = X2f_6_items,
         "2D, 5 items" = X2f_5_items)

kable(fit_measures, 
      "latex", 
      booktabs = T,
      caption="CFA fit measures of general (G) and immigration (I) knowledge questions for unidimensional (1D) and two-dimensional (2D) models", 
      label="tab3")
```

Reading across these sets of analyses, the EFA results suggest that the knowledge questions are potentially tapping into distinctive dimensions. This is not surprising, given that they were intended to measure different types of knowledge. At the same time, while the CFA results indicate that the two-dimensional models display superior fits, the fact that the unidimensional models also exhibit very good fits suggests that the generalist assumption is not obviously misguided at this stage. This is in line with @zaller1986's original conclusion that the difference in performance between general and issue-specific scales are likely to be small enough and rare enough to be consistent with political knowledge being "a highly general trait."

## Step 2: Construct Validation {#construct-validation}

Factor analysis enables us to investigate the extent to which correlations between items can be plausibly explained with reference to a certain number of factors, or underlying traits. However, such analysis does not speak to whether the trait thereby measured is the intended one, i.e. to the matter of *construct validity*. For this reason, we now turn attention to whether the correlations between each of the two knowledge scales and a variety of demographic variables appear the way we would expect if these scales were measuring forms of knowledge. Moreover, we examine whether the correlations exhibited by the two scales are similar, as they should be if the generalist assumption holds.

To do this, we first create two knowledge scales using IRT modeling [@deayala2009; @demars2010]. IRT models measure latent traits assumed to fall on a continuous scale. Values on that scale are usually referred to by way of $\theta$ (theta), and taken to range from -$\infty$ to +$\infty$, with a mean of 0 and standard deviation of 1. This means that, while the individual $\theta$ value ascribed to any particular respondent has no intrinsic meaning, it can nevertheless be interpreted relative to an estimated population mean. Full details on the IRT models used in this section as well as those used subsequently can be found in the Supporting Information.[^4] 

[^4]: We account for the possibility of guessing by fitting separate three-parameter models for the general and immigration-specific knowledge scales (see Supporting Information). These results indicate a very low likelihood of guessing.

Having ensured that the two sets of items satisfy standard assumptions for an IRT model (unidimensionality, local independence, and acceptable model fit), we then regress these scales on gender, age, and education, across the pooled REMINDER data set (i.e. not segmenting by nationality). The first two variables are self-explanatory. In the data set, the education variable takes on a value from 0 to 8, corresponding to the nine International Standard Classification of Education (ISCED) levels of education, from early childhood education (0) to doctorate (8). Details about the resulting models can be found in Table \ref{tab:4}.

```{r tab4, echo=FALSE, results="asis"}
m_gen <- lm(know_score_general ~
              gender + # men know more
              age + # older know more
              education_ISCED, # more educated know more
            data = reminder_data)

m_imm <- lm(know_score_imm ~
              gender + # men know more
              age + # older know more
              education_ISCED, # more educated know more
            data = reminder_data)

texreg(list(m_gen, m_imm),
               single.row = TRUE,
               caption = "Regression models (OLS)",
               custom.model.names=c("General knowledge",
                                    "Immigration knowledge"),
               custom.coef.names = c("(Intercept)", "Male", "Age", "Education"),
               caption.above = TRUE,
               float.pos = "h!",
               custom.note="%stars.",
               stars = c(0.001, 0.01, 0.05),
               digits=3,
               label="tab:4")
```

Table \ref{tab:4} shows how the coefficient values in both cases cohere with what we know about the relationship between political knowledge and demographics: men tend to have more political knowledge than women [@vanheerdehudson2020; @plutzer2020; @carpinikeeter1996], and the educated more political knowledge than the uneducated [@hebbelstrup2016]. This echoes the patterns found by @barabas2014 in their study of knowledge items. It also seems reasonable to assume that political knowledge increases with age, as indeed is the case on both models. Moreover, as can also be seen from the table, the associations are very similar between the two scales, which speaks in favor of unidimensionality: both scales appear to be measuring the same fundamental, underlying trait.

As a further test of construct validity, we compare these two scales with other widely-used measures of political knowledge from the American National Elections Study (ANES), the Cooperative Election Study (CES), and the British Election Study (BES). All of the question batteries we report here form knowledge scales that satisfy standard IRT modeling assumptions (see Supporting Information for details). The ANES data set is the 2019 Pilot Study [@anes2019] (N = 3,000), which contains the following knowledge items:

> (ANES1) What job or political office is now held by John Roberts? (Correct: Chief Justice of the U.S. Supreme Court)\
> (ANES2) What job or political office is now held by Angela Merkel? (Correct: Chancellor of Germany [in 2019])\
> (ANES3) For how many years is a United States Senator elected -- that is, how many years are there in one full term of office for a U.S. Senator? (Correct: 6 years)

The CES data set [@ansolabehere2021] (N = 60,138) contains the following knowledge items:

> Please indicate whether you've heard of this person and if so which party he or she is affiliated with (response options: Never heard of person; Republican; Democrat; Other Party/Independent; Not sure):\
> (CES1) *Name of Governor of respondent's state*.\
> (CES2) *Name of first US Senator of respondent's state*.\
> (CES3) *Name of second US Senator of respondent's state*.\
> (CES4) *Name of one member of the US House of Representatives of respondent's state*.

Finally, in the case of BES, we use the 2017 Post-Election Face-to-Face Survey [@fieldhouse2018] (N = 2,067)[^5] which contains the following knowledge items:

> (BES1) No-one may stand for parliament unless they pay a deposit (True)\
> (BES2) The Liberal Democrats favour a system of proportional representation (True)\
> (BES3) MPs from different parties are on parliamentary committees (True)\
> (BES4) The number of members of parliament is about 100 (False)

[^5]: The original data set contains 2,194 observations. 127 observations with missing survey weights were removed, for a resulting sample size of 2,067.

These questions are valuable for our stress-testing objective because, while all general in the sense defended in section \ref{conceptual-issues}, they vary in terms of their temporal dimension [@barabas2014]. For example, some items ask about facts regarding well-established political processes or features (e.g. ANES3, BES1-4), while others require a degree of recent surveillance on the part of respondents (e.g. ANES1, ANES2, CES1-4). Considering these differences matters because they mirror theoretical expectations of how knowledge informs political behavior. On the one hand, knowledge of fundamental and unchanging political rules potentially enables citizens to actively participate in politics [@carpinikeeter1996]. On the other hand, knowing more recent facts potentially signals an ability to learn about -- and have views on -- political events as they happen [@schudson2011].

As the education variables differ between the REMINDER data set used above, and the BES, ANES, and CES data sets (the latter three use standard U.K./U.S. education categories rather than ISCED), we investigate the relationship between these knowledge scales and aforementioned demographic characteristics by plotting and comparing patterns of estimated marginal means, seen in Figures \ref{fig:emmeans_plots1} and \ref{fig:emmeans_plots2}.

```{r preprocess_for_plot, echo=FALSE, message=FALSE, results=FALSE}
# function to calculate and plot marginal means
emmeans_function <- function(model, covariates, xlabs) {
  plot_list <- list()
  for (i in 1:length(covariates)) {
    emmeans_object <- summary(emmeans(model, specs = covariates[i]))
    p <- ggplot(emmeans_object) +
      aes_string(x = covariates[i], y = "emmean") +
      geom_line(group=1, color = cbPalette[1]) +
      geom_pointrange(aes(ymin=lower.CL, ymax=upper.CL), color = cbPalette[1]) +
      geom_hline(yintercept=0, linetype="dashed", alpha=0.5) +
      xlab(xlabs[i]) +
      ylab("Estimated marginal mean") +
      theme(text = element_text(size=global_font_size)) +
      theme_minimal()

    plot_list[[i]] <- p
  }
  return(ggarrange(plotlist = plot_list, nrow=1))
}

reminder_data <- reminder_data %>% 
  mutate(education_ISCED = factor(education_ISCED,
                                  ordered = T),
         age_cat = cut(age, breaks=c(18, 24, 34, 44, 54, 64, Inf), 
                       labels=c("18-24","25-34","35-45","45-54","55-64","65+"),
                       ordered_result = TRUE),
         gender = recode(gender,
                         "male" = "Male",
                         "female" = "Female"))

m_reminder_general <- lm(know_score_general ~
                           age_cat +
                           gender +
                           education_ISCED,
                         data = reminder_data)

m_reminder_imm <- lm(know_score_imm ~
                       age_cat +
                       gender +
                       education_ISCED,
                     data = reminder_data)

# benchmark data 1: bes wave 17
bes_data <- bes_data %>% 
  mutate(education = recode(education,
                            `0` = "None",
                            `1` = "<GCSE",
                            `2` = "GCSE",
                            `3` = "A-lvl.",
                            `4` = "UG",
                            `5` = "PG"),
         education = factor(education,
                            levels = c("None",
                                       "<GCSE",
                                       "GCSE",
                                       "A-lvl.",
                                       "UG",
                                       "PG"),
                            ordered = T),
         age = recode(age,
                      `1` = "18-24",
                      `2` = "25-34",
                      `3` = "35-44",
                      `4` = "45-54",
                      `5` = "55-64",
                      `6` = "65+",
                      `7` = "65+",
                      `8` = "65+"),
         gender = recode(gender,
                         `1` = "Male",
                         `2` = "Female"))

m_bes <- lm(ability ~
              age +
              gender +
              education,
            data = bes_data)

# benchmark data 2: anes 2019
anes_data <- anes_data %>% 
  mutate(educ = factor(educ,
                       levels = c("no_hs",
                                  "high_school",
                                  "some_college",
                                  "two_yr_college",
                                  "four_yr_college",
                                  "post_grad"),
                       ordered = T),
         educ = recode(educ,
                       "no_hs" = "None",
                       "high_school" = "HS",
                       "some_college" = "Some coll.",
                       "two_yr_college" = "2yr",
                       "four_yr_college" = "4yr",
                       "post_grad" = "PG"),
         gender = recode(gender,
                         `1` = "Male",
                         `0` = "Female"),
         age_cat = cut(age, breaks=c(18, 24, 34, 44, 54, 64, Inf), 
                       labels=c("18-24","25-34","35-45","45-54","55-64","65+"),
                       ordered_result = TRUE))

m_anes <- lm(ability ~
               age_cat +
               gender +
               educ,
             data = anes_data)

# benchmark data 3: cces 2020
cces_data <- cces_data %>% 
         mutate(educ = factor(educ,
                       levels = c("None",
                                  "HS",
                                  "Some coll.",
                                  "2yr",
                                  "4yr",
                                  "PG"),
                       ordered = T),
                age_cat = factor(age_cat, 
                          levels=c("18-24",
                                   "25-34",
                                   "35-45",
                                   "45-54",
                                   "55-64",
                                   "65+"),
                       ordered = T))

m_cces <- lm(knowledge ~
               age_cat +
               gender +
               educ,
             data = cces_data)

# covid data
covid_data <- covid_data %>% 
  mutate(education = factor(education, 
                            levels=c("No formal education",
                                     "GCSE (or equivalent)",
                                     "A-level (or equivalent)",
                                     "Undergraduate degree (e.g., BA)",
                                     "Postgraduate degree (e.g., MA, MSc, PhD)"),
                            ordered = T),
         education = recode(education,
                            "No formal education" = "None",
                            "GCSE (or equivalent)" = "GCSE",
                            "A-level (or equivalent)" = "A-level",
                            "Undergraduate degree (e.g., BA)" = "UG",
                            "Postgraduate degree (e.g., MA, MSc, PhD)" = "PG"),
         age_cat = cut(age, breaks=c(18, 24, 34, 44, 54, 64, Inf), 
                       labels=c("18-24","25-34","35-45","45-54","55-64","65+"),
                       ordered_result = TRUE))

m_covid <- lm(know ~
                age_cat +
                gender +
                education,
              data = covid_data)

# epcc data
epcc_data <- epcc_data %>% 
  filter(education != "other" & education != "student") %>% 
  mutate(education = factor(education, 
                            levels=c("no_qual","gcse","alevel","degree"),
                            ordered = T),
         education = recode(education,
                            "no_qual" = "None",
                            "gcse" = "GCSE",
                            "alevel" = "A-level",
                            "degree" = "Degree"),
         gender = recode(gender,
                         "male" = "Male",
                         "female" = "Female"),
         age = factor(age,
                      levels = c("15-24",
                                 "25-34",
                                 "35-44",
                                 "45-54",
                                 "55-64",
                                 "65+"),
                      ordered = T))

m_epcc <- lm(knowledge ~
               age +
               gender +
               education,
             data = epcc_data)
```

```{r emmeans_plots1, fig.cap = "Estimated marginal means for REMINDER knowledge scales", echo=FALSE, fig.width=11,fig.height=7}
p1 <- emmeans_function(m_reminder_general,
                 c("education_ISCED","gender","age_cat"),
                 c("Highest level of education (ISCED)","Gender","Age bracket"))

p2 <- emmeans_function(m_reminder_imm,
                 c("education_ISCED","gender","age_cat"),
                 c("Highest level of education (ISCED)","Gender","Age bracket"))

ggarrange(plotlist = list(annotate_figure(p1, top = text_grob("REMINDER: General knowledge", 
               color = "black", size = 16)),
                          annotate_figure(p2, top = text_grob("REMINDER: Immigration knowledge", 
               color = "black", size = 16))),
          nrow=2)
```

```{r emmeans_plots2, fig.cap = "Estimated marginal means for BES and ANES knowledge scales", echo=FALSE, fig.width=11,fig.height=10.5}

p3 <- emmeans_function(m_anes,
               c("educ","gender","age_cat"),
                 c("Highest level of education","Gender","Age bracket"))

p4 <- emmeans_function(m_cces,
                 c("educ","gender","age_cat"),
                 c("Highest level of education","Gender","Age bracket"))

p5 <- emmeans_function(m_bes,
                 c("education","gender","age"),
                 c("Highest level of education","Gender","Age bracket"))

ggarrange(plotlist = list(annotate_figure(p3, top = text_grob("ANES", 
               color = "black", size = 16)),
               annotate_figure(p4, top = text_grob("CES", 
               color = "black", size = 16)),
               annotate_figure(p5, top = text_grob("BES", 
               color = "black", size = 16))), 
          nrow=3)
```

Figure \ref{fig:emmeans_plots1} shows that the estimated marginal means for each of the three demographic variables are very similar across the general and migration-specific knowledge scales. This is consistent with the evidence from previous sections for the generalist assumption: if both scales are tapping into the same underlying trait, then we should indeed expect the marginal means to exhibit similar patterns across these variables, as they do.

Turning to Figure \ref{fig:emmeans_plots2}, the fact that the two scales from the previous figure exhibit similar patterns in their marginal means to the three established scales from the questions contained within the ANES, CES, and BES offers further evidence for both the generalist assumption and for construct validity, i.e. for the scales tapping into a particular form of knowledge.

Finally, as shown in Figures \ref{fig:emmeans_plots3a} and \ref{fig:emmeans_plots3b}, breaking down the estimated means from the REMINDER scales by country suggests that the observations made about the pooled data appear to hold in the cases of individual countries, too. This offers further evidence that the generalist assumption is robust in the face of geographic segmentation.

```{r emmeans_plots3a, fig.cap = "Estimated marginal means for general knowledge scale by country", echo=FALSE, fig.width=11,fig.height=16}

means_by_nationality <- function(nationalities, knowledge_var) {
  plot_list <- list()
  for (i in 1:length(nationalities)) {
    m <- lm(glue::glue("{knowledge_var} ~ age_cat + gender + education_ISCED"),
            data = subset(reminder_data, nationality==nationalities[i]))
    plot <- emmeans_function(m,
                             c("education_ISCED","gender","age_cat"),
                             c("Highest level of education","Gender","Age bracket"))
    plot_list[[i]] <- annotate_figure(plot, top = text_grob(str_to_upper(nationalities[i], locale = "en"), color = "black", size = 12))
  }
  return(ggarrange(plotlist = plot_list, nrow=7))
}

means_by_nationality(c("germany","hungary","poland","romania","spain","sweden","uk"), "know_score_general")
```

```{r emmeans_plots3b, fig.cap = "Estimated marginal means for immigration knowledge scale by country", echo=FALSE, fig.width=11,fig.height=16}

means_by_nationality(c("germany","hungary","poland","romania","spain","sweden","uk"), "know_score_imm")
```

## Step 3: Extending the Generalist Assumption beyond Migration

So far, we have provided evidence for the generalist assumption in the context of general political knowledge versus migration-specific knowledge. However, our analysis cannot rule out that there is something unique about migration-specific knowledge, perhaps in it having an especially close connection to general political knowledge. Therefore, in our third step, we compare the marginal means of the two REMINDER knowledge scales to those of two, separate issue-specific scales: one relating to public health (specifically knowledge about COVID-19) and one relating to knowledge about climate change. These topics, and the questions within them, are useful for stress-testing the generality of our results from the previous section because while they both deal with policy-specific domains, they vary in terms of temporal dimension just as the previous general knowledge questions from the ANES, CES, and BES do [@barabas2014]. Facts about climate change have arguably become well-established in public consciousness, whereas possessing knowledge about COVID-19 -- at the time of the survey fieldwork -- would likely indicate a degree of active surveillance by respondents.

Our COVID-19 data set comes from a a pre-registered survey experiment (N = 2,917) fielded in the U.K. through the wake of its first pandemic wave in July 2020. The survey contains the following knowledge items, measured pre-treatment, which together form a scale satisfying standard assumptions for IRT (again, see the Supporting Information document for further details on the IRT models):

> (COV1) COVID-19 can be transmitted in areas with hot and humid climate (True)\
> (COV2) There is currently no vaccine to protect against COVID-19 (True [in July 2020])\
> (COV3) Most people who get COVID-19 recover from it (True)

The climate change data set is from the U.K. segment (N = 1,033) of the European Perceptions of Climate Change survey [@pidgeon2016]. The survey was not specifically designed to measure knowledge about climate change, but it does contain three items which could reasonably be taken to tap into such knowledge,[^6] and moreover form a scale satisfying standard assumptions for IRT:

[^6]: As (CC2) is formulated in terms of the respondent's opinion, it might be taken not to measure someone's level of knowledge. However, as the item asks about the respondent's opinion on a matter of fact, and any opinion taken by the respondent thereby can be deemed correct or incorrect, we treat it as a knowledge item.

> (CC1) As far as you know, do you think the world's climate is changing or not? (Correct: The climate is changing)\
> (CC2) Thinking about the causes of climate change, which, if any, of the following best describes your opinion? (Correct: Climate change is partly, mainly, or completely caused by human activity)\
> (CC3) To the best of your knowledge, what proportion of scientists agree that climate change is happening and that humans are largely causing it? (Correct: The vast majority of scientists agree, at 80% or more)

```{r emmeans_plots4, fig.cap = "Estimated marginal means for COVID and Climate Change knowledge scales", echo=FALSE, fig.width=11,fig.height=7}
p6 <- emmeans_function(m_covid,
                 c("education","gender","age_cat"),
                 c("Education","Gender","Age bracket"))

p7 <- emmeans_function(m_epcc,
                 c("education","gender","age"),
                 c("Highest level of education","Gender","Age bracket"))

ggarrange(plotlist = list(annotate_figure(p6, top = text_grob("COVID knowledge", 
               color = "black", size = 16)),
                          annotate_figure(p7, top = text_grob("Climate change knowledge", 
               color = "black", size = 16))), 
          nrow=2)
```

Figure \ref{fig:emmeans_plots4} plots the estimated marginal means for the same demographic variables as in the previous section for the COVID and climate change knowledge scales. For both scales, the marginal means by education shows the same, monotonic trend seen in Figure \ref{fig:emmeans_plots1} and \ref{fig:emmeans_plots2}. In the case of both scales, men generally have more knowledge than women, which again aligns with what we found in regards to the scales in the previous section. In the case of age, we see a more pronounced drop from the first to the second bracket than what we saw for the previous four scales. Additionally, the climate change scale in particular exhibits a less straightforward relationship than the other scales, in having substantially higher levels of knowledge in the first bracket. It is worth reflecting on whether knowledge about climate change might have a different age distribution given the potential for higher levels of awareness among younger compared to older people, in light of how the former have a greater number of years of exposure to the effects of climate change ahead of them.

# Discussion and Practical Guidance

A long line of research demonstrates how being more politically informed potentially matters for a host of outcomes, including what people think and prefer about issues. However, whether on principled or practical grounds, much of this work invokes what we have called the *generalist assumption*: that knowledge about general political topics is indicative of knowledge about issue-specific topics. From a measurement perspective, this assumption is useful because it allows researchers to deploy necessarily limited question batteries as proxies for other unmeasured forms of knowledge that are actually of interest. Yet despite its centrality to core public opinion work, this assumption is difficult to empirically test owing to the lack of surveys that contain general and specific knowledge questions. Having evidence of whether and in which circumstances it holds would present implications for research that uses knowledge as an explanatory variable.

In response, we have exploited a recent, large-scale survey that, unusually, contains both types of items in one instrument across seven European countries. Combining several analytical approaches, we conclude that the preponderance of evidence points to the generalist assumption being defensible. First, exploratory and confirmatory factor analyses applied to various combinations of the knowledge items revealed how the collection of questions performed well when treated unidimensionally. Second, in terms of construct validity, key demographic features that prior work leads us to expect are relevant to political knowledge exhibit similar associations between our political and immigration knowledge scales. Moreover, both scales in the aggregate and by country exhibit similar patterns in estimated marginal mean level of knowledge to established knowledge scales built from questions in the ANES, CES, and BES. Third, in an extension to migration, we observe the same patterns in the case of issue-specific knowledge scales about COVID-19 and climate change which reflect knowledge based in both longer- and shorter-term events.

Our study has practical implications for the ways that researchers use political knowledge questions as they study domain-specific attitudes and preferences. Although we argue the generalist assumption is defensible, we also emphasize that it should not be deployed uncritically (as reflected in our discussion of the results regarding the age-related correlates of knowledge about climate change in Figure \ref{fig:emmeans_plots4}, and by our acknowledgement in footnote 1 of how "standard" knowledge questions display limitations). Rather, we advise that researchers should stress-test it as far as available data sets allow, notably by including knowledge items from different domains, and by using a combination of methods to evaluate its plausibility in specific contexts. Having evidence of this would be useful in at least two situations: where researchers want to evaluate the dimensionality of knowledge items available to them to decide which scales to use; and where researchers are collecting data, want to run a pilot with a range of knowledge items, and then choose the minimum number of items needed to achieve their objectives to potentially save funds and survey time.

Therefore, we offer the following guidelines informed by our analysis:

1.  Start by conducting a parallel analysis on the total set of knowledge items, and then investigate the loadings from an exploratory factor analysis (EFA) on those same items, where the number of factors assumed in the EFA is informed by the parallel analysis.\
2.  Let the EFA results inform a confirmatory factor analysis (CFA) that compares a unidimensional model to one or several multidimensional models, depending on the number of factors suggested by the EFA, and in order to see whether the former exhibits a good enough fit to compete with the multidimensional model(s). If it does, this would point towards unidimensionality.\
3.  Construct two or more knowledge scales, depending on the dimensionality under investigation, and then use each scale as an outcome in a regression model, with gender, education, and age as predictors. See if there any similarities in coefficient values between the two (or more) models. High similarity across all predictors would count towards unidimensionality.\
4.  Estimate marginal mean levels of knowledge by gender, education, and age, respectively, for each of the scales, and compare the patterns in these levels to those exhibited in Figures \ref{fig:emmeans_plots1}, \ref{fig:emmeans_plots2} and \ref{fig:emmeans_plots4}. Similarity with the patterns found there would count towards both construct validity and unidimensionality.\

For any given project, all or only some of these steps will be appropriate or possible. However, it is our hope that the above steps will nonetheless offer helpful guidance for researchers concerned with questions about knowledge scale dimensionality. To facilitate replication, and in line with Open Science principles, we also provide the code underlying the present investigation as a template for others to use in a public GitHub repository.[^7] 

[^7]: The repository can be accessed at https://github.com/ahlstromvij/REMINDER_project.

# References
