---
# This template is largely functionally equivalent to that for elsevier
# The two fields that are not in the elsevier format are:
#      `corresponding_author` and `acknowledgements`
fontsize: 12pt
title: |
  Is possessing general political knowledge diagnostic of holding issue-specific knowledge? 
  Stress-testing the generalist assumption
author:
  - name: William L. Allen
    email: william.allen@politics.ox.ac.uk
    corresponding_author: yes # Footnote using corresponding_author
  - name: Kristoffer Ahlstrom-Vij
    email: k.ahlstrom-vij@bbk.ac.uk
footnote:
  - code: 1
    text: "Current email address: \\href{mailto:cat@example.com}{cat@example.com}."
abstract: |
 The practice of using general political knowledge questions to explain variation in policy-specific outcomes invokes a potentially strong generalist assumption: possessing general knowledge is diagnostic of holding issue-specific knowledge. Is this warranted? Using convergent evidence drawn from best-available US and European surveys, we argue it is. First, factor analysis applied to a cross-national 2018 European data set that, unusually, includes questions about general politics and the specific issue of EU immigration reveals how combining the questions produces a plausibly unidimensional set. Second, knowledge scales built using Item Response Theory (IRT) modeling of responses to each set of questions display similar associations with known correlates of political knowledge--gender, age, and education--as well-established ANES, CES, and BES knowledge measures, including when broken down by country. Finally, these patterns remain after extending our analysis to surveys that test knowledge about public health (specifically COVID-19) and climate change. We propose steps for conducting similar stress-tests on unidimensionality assumptions.

acknowledgements: |
  WA acknowledges support from the British Academy (grant number PF21\210066).
keywords:
  - political knowledge
  - immigration
  - public health
  - climate change
#fontsize: 12pt
#spacing: halfline # could also be oneline
#classoptions:
#  - endnotes
bibliography: mybibfile.bib
#csl: mycitationstyle.csl
#link-citations: yes
output: rticles::oup_article
#urlcolor: orange
#linkcolor: green
#citecolor: red
#header-includes:
#  - \usepackage[nomarkers,tablesfirst]{endfloat} # For figures and tables at end
#  - \usepackage{lineno} # For line numbering
#  - \linenumbers # For line numbering
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE) # By default, hide code; set to TRUE to see code
knitr::opts_chunk$set(fig.pos = '!h') # Places figures in text
knitr::opts_chunk$set(out.width = '100%', dpi=300) # Figure resolution and size
knitr::opts_chunk$set(fig.env="figure") # Latex figure environment

library(tidyverse)
library(kableExtra)
library(psych)
library(lavaan)
library(texreg)
library(emmeans)
library(ggpubr)

# global plotting values
global_font_size = 14
# color blind friendly palette
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

reminder_data <- read_csv("../../data/model_data_IRT.csv")
```

# Introduction

Political knowledge has been called a "workhorse variable" [@kleinberg2022] owing to its usefulness as a proxy for other concepts, including political sophistication and competence [@carpinikeeter1996]. However, survey-based practice has operationalized this variable in different ways, which potentially threatens conclusions about both its determinants and consequences. In their overview of 335 knowledge questions fielded between 2007-10, @barabas2014 identify two dimensions that characterize these questions: a temporal dimension capturing how recently the factual information emerged; and a topical dimension capturing whether the factual information refers to general institutions or actors, or specific public policy issues. In particular, they focus on the *temporal* dimension of their typology to examine the mechanisms by which voters acquire knowledge.

By contrast, our interest lies in the *topical* dimension of political knowledge questions--specifically in assessing how general and policy-specific knowledge relate, and what this implies for researchers' necessarily constrained efforts to effectively and efficiently measure voters' levels of knowledge. Such an assessment matters because a dearth of issue-specific knowledge items in established surveys means that researchers routinely try to explain variation in policy-specific attitudes or behaviors using general knowledge items intended to distinguish between more- and less-informed respondents, and in so doing are forced to make a potentially strong assumption: possessing general political knowledge is diagnostic of holding issue-specific knowledge (and _vice versa_). We call this the *generalist assumption*. 

In various forms, the idea that people who are knowledgeable in one area of politics will be knowledgeable in others as well has underpinned much of the key work in public opinion [@zaller1986; @zaller1992; @carpinikeeter1996]. Methodologically, this is attractive because it implies that researchers can use a general knowledge scale to measure the extent to which people are politically informed across specific domains, even in the absence of items directly concerned with the particular topics represented within those domains. Yet despite its methodological and practical centrality, this assumption has remained largely untested, partly due to the lack of available survey instruments which contain both general and issue-specific factual items. 

In response, we present convergent evidence that stress-tests the generalist assumption by applying a variety of methods to a range of the best-available survey evidence spanning country contexts (the US and seven European countries), topics (immigration, public health, and climate change), and respondent panel type (established omnibus and bespoke online panels). Among other steps, we leverage cross-national European survey data that, unusually, contains questions about both general politics and a specific issue, namely European Union immigration [@meltzer2020]. By assessing the dimensionality and construct validity of the range of knowledge questions contained within our assembled surveys, and by comparing these features to widely-used instruments (the American National Elections Studies, the Cooperative Election Study, and the British Election Study), we conclude that the generalist assumption holds up to scrutiny. Besides contributing novel empirical evidence for this common methodological practice, we also offer practical advice for researchers who either want to conduct similar stress-tests of their own topic- and context-specific survey data on grounds of unidimensionality assumptions, or are considering which types of knowledge questions to include in future studies while acknowledging concerns about resources, time, and survey attentiveness.

# The Generalist Assumption about Political Knowledge

The generalist assumption can be traced back at least to John Zaller's analysis of information items in the 1985 National Election Study Pilot, where he argued that "political information is a relatively general trait that can be effectively measured with a general-purpose information scale" [@zaller1986, 2]. This conclusion subsequently informed his landmark study on public opinion, where he noted that he was "assuming that persons who are knowledgeable about politics in general are habitually attentive to communications on most particular issues as well" [@zaller1992, 43]. @carpinikeeter1996 investigated and defended that assumption, which later featured in both @bartels1996 and @althaus2003's influential works "information effects," i.e., the extent to which voters would express different preferences had they possessed more information. 

Distinguishing between more- and less-informed respondents is a central problem to research that uses knowledge items. On the one hand, this decision could be based on scales that measure respondents' levels of correctness to question batteries, as @carpinikeeter1996 do. On the other hand, this could be inferred from interviewer ratings of respondents' "general level of information about politics and public affairs" [@bartels1996, 203]. Subsequent work across a variety of geographical contexts including the US [@ahlstromvij-posttruth], the UK [@ahlstromvij2020-moddemo], Denmark [@bhatti2010; @hansen2009], Sweden [@oscarsson2007] and Canada [@blaisetal2009] follows the norms set by these early studies by using either a variety of general knowledge items or a scale of both general and issue-specific knowledge items alongside interviewer ratings. 

As such, the generalist assumption has come to carry an increasingly heavy burden: not only is the catalogue of geographical contexts in which researchers investigate the impacts of knowledge on political attitudes and choice expanding, but so also is the list of the specific policy issues that feature in models of voters' attitudes and preferences. Yet, as @barabas2014 identify in their assessment of 355 knowledge items fielded between 2007-10, survey researchers have tended to use questions that test respondents' familiarity with either relatively unchanging aspects of government institutions (107 items comprising 30% of their sample) or details of policy-specific current affairs (113 items comprising 32% of their sample). Consequently, researchers using responses to knowledge items as secondary data, or considering fielding their own items as part of original data collection efforts, will have to use a necessarily limited set of questions -- likely varying along the general-specific dimension -- as a proxy for what respondents know about some other domain that is *actually* of interest. 

Hence, the substantial weight put on the generalist assumption in current scholarship. The question we set ourselves is whether that assumption holds. If it does, we expect that different methods of analysis applied to a range of survey data and knowledge questions (described later) will converge to support three core expectations: (1) relevant patterns of responses on observed knowledge variables can be plausibly represented by a single latent trait (factor); (2) the relevant knowledge scales (both general and issue-specific) should exhibit similar associations with demographic variables found by prior work to correlate with political knowledge, and in that regard also resemble knowledge scales derived from established political attitudes surveys; and (3) those associations should coincide with equivalent analyses applied to knowledge about other, issue-specific matters.

# Methods and Data

## Dataset

For our main analysis, we use a subset of online survey data collected as part of the REMINDER (Role of European Mobility and Its Impacts in Narratives, Debates and EU Reforms) project [@meltzer2020].[^1] Unlike many existing data sets, this survey has two features which make it uniquely valuable for our purposes. First, it contains both general political knowledge questions *and* issue-specific questions -- in this case, on migration -- in the same survey instrument. This allows us to assess the extent to which these items are collectively unidimensional, as well as whether both sets have similar demographic correlates. Second, it is cross-national, covering seven European countries (Germany, Hungary, Poland, Romania, Spain, Sweden, and the United Kingdom). This enables us to examine whether any results on unidimensionality are artifacts of particular national profiles, e.g. the United States, on which early studies into the unidimensionality of political knowledge were based.

[^1]: Full details about the data set, including its documentation and questionnaire design, are available at <https://doi.org/10.11587/LBSMPQ>.

The REMINDER study used a panel design comprising three waves across 2017-18, whose sampling procedures used quotas by age, gender, and region (at NUTS2 level) to approximate representativeness for each country's adult population. We use data from the second wave (collected between June 6 and July 16, 2018) because our items of interest were only asked in that wave alongside the two sets of knowledge questions. Our sample covers 10,749 respondents, with a breakdown of demographic details appearing in Table \ref{tab:tab1}.

```{r tab1, echo=FALSE, results="asis"}
# create table 1
tbl1 <- reminder_data %>% 
  group_by(nationality) %>% 
  mutate(nationality = str_to_sentence(nationality),
         nationality = recode(nationality,
                              "Uk" = "UK")) %>% 
  rename(Nationality = nationality) %>% 
  summarise("Mean age" = round(mean(age, na.rm=T),0),
            Male = sum(gender=="male"),
            Female = sum(gender=="female"),
            "ISCED 0" = sum(education_ISCED==0),
            "ISCED 1" = sum(education_ISCED==1),
            "ISCED 2" = sum(education_ISCED==2),
            "ISCED 3" = sum(education_ISCED==3),
            "ISCED 4" = sum(education_ISCED==4),
            "ISCED 5" = sum(education_ISCED==5),
            "ISCED 6" = sum(education_ISCED==6),
            "ISCED 7" = sum(education_ISCED==7),
            "ISCED 8" = sum(education_ISCED==8),
            N = n()
  )

kable(t(tbl1), "latex", booktabs = T, caption="Demographic details of the sample", label="tab1") %>%
  kable_styling(latex_options = "scale_down") %>% 
  row_spec(1, bold = T, hline_after = T) %>% 
  row_spec(13, hline_after = T)
```

## General and Specific Knowledge

To measure general political knowledge, we identify correct responses to the following statements:

> (G1) "Switzerland is a member of the EU" (False);\
> (G2) "Every country in the EU elects the same number of representatives to the European Parliament" (False); and\
> (G3) "[NAME OF THE HEAD OF GOVERNMENT] belongs to the [NAME OF CORRECT PARTY]", depending on the survey country.

Then, to measure migration-specific knowledge, we identify correct responses to the following statements:

> (I1) "The free movement of persons is a fundamental right guaranteed by the EU to its citizens" (True);\
> (I2) "Greece is part of the Schengen Area" (True);\
> (I3) "In 2015, Afghans have been the largest group of people that applied for asylum in the EU" (False); and\
> (I4) "In 2015, asylum in the EU was more frequently granted to Syrians than any other nationality" (True).

## Methods

To evaluate the core elements of the generalist assumption, we apply a three-step approach that expands on that taken by @carpinikeeter1996 in their early work on knowledge scales. First, we use a combination of exploratory (EFA) and confirmatory (CFA) factor analysis to investigate the dimensionality of the political knowledge response data.

Second, we use regression analysis to investigate whether any latent traits exhibit correlations with demographic traits that would be expected if they corresponded to political knowledge (a measure of construct validity). To do this, we go beyond @carpinikeeter1996 in using Item Response Theory (IRT) in both creating and validating general and immigration-specific knowledge scales based on the batteries of questions available in the data set [@deayala2009; @demars2010], and the calculatin the estimated marginal mean levels of general political knowledge across levels of education, gender, and age as contained in three reference data sets -- the American National Elections Studies pilot [@anes2019], the Cooperative Election Study [@ansolabehere2021], and Wave 17 of the British Election Study [@fiedhouse2020] -- to investigate whether these exhibit similar patterns as those found in the REMINDER data. 

Third, we replicate this analysis using scales independently derived from surveys that contain knowledge items about other, issue-specific matters, namely public health (specifically about COVID-19) and climate change. The COVID-19 questions were part of a bespoke survey experiment fielded in the UK (N = 2,917), while the climate change questions came from a survey fielded by @pidgeon2016. This step addresses concerns that our results are merely artifacts of migration knowledge somehow being uniquely related to general political knowledge.

# Empirical Analysis

## Step 1: Exploratory and Confirmatory Factor Analysis

First, we combine all six items from both knowledge batteries, and conduct a parallel analysis to estimate how many factors are likely involved. This suggests four factors in our case. We then apply exploratory factor analysis (EFA) set to four factors. As can be seen from Table \ref{tab:tab2}, three of the immigration items (I1, I2, and I4) load well onto one factor (Factor 2) while two of the general items (G1 and G2) load well onto another (Factor 4). Further parallel analysis of the three general items (G1-3) suggests one factor, and parallel analysis for the three immigration items does the same, if excluding I3. This offers initial evidence of two separate factors, corresponding to general knowledge and immigration knowledge, respectively.[^2]

[^2]: We do not look at scale reliability as measured by Chronbach's alpha, since such reliability can be high even given multidimensionality [@fabrigar2011].

```{r tab2, echo=FALSE, results="asis"}
fa_tibble <- tibble(
  Item = c("G1","G2","G3","I1","I2","I3","I4"),
  "Factor 1" = c(0.121,0.130,"","","",1.002,0.385),
  "Factor 2" = c(0.139,0.207,"",0.708,0.632,"",0.378),
  "Factor 3" = c(0.210,"",1.000,"","","",0.161),
  "Factor 4" = c(0.529,0.456,"","","","",-0.237)
  )

kable(fa_tibble, 
      "latex", 
      booktabs = T, 
      caption="EFA of General (G) and Immigration (I) Knowledge Questions", 
      label="tab2")
```

Next, we probe further by using confirmatory factor analysis (CFA) to compare the fit of four models against each other: two unidimensional ones and two two-dimensional ones, with the first one in each pair containing all six items and the second one excluding one of these (G3, i.e. the one that failed to load well onto Factor 4 in the earlier EFA). As shown in Table \ref{tab:tab3}, the model with the best fit is two-dimensional and with five items, but the unidimensional models also show very good fit. For context, Root Mean Square Error of Approximation (RMSEA) values of less than 0.05 and 0.01 usually are taken to indicate good and very good fit, respectively [@andrews2021]. A Comparative Fit Index (CFI) or Tucker-Lewis Index (TLI) greater than 0.95 [@dima2018], and an Adjusted Goodness-of-fit Index (AFGI) greater than 0.9, are typically taken to suggest good fit [@baumgartner1996].

```{r tab3, echo=FALSE, results="asis"}
mod_1f_6_items <- 'knowledge =~ gen_know_ep + gen_know_party + gen_know_switzerland + mig_know_free_move + mig_know_schengen + mig_know_syrians'
mod_1f_6_items.fit <- cfa(mod_1f_6_items, data=reminder_data, ordered = TRUE)

fit_measures <- data.frame("Measure" = c("rmsea","cfi","tli","agfi"),
                           "1f_6_items" = NA,
                           "1f_5_items" = NA,
                           "2f_6_items" = NA,
                           "2f_5_items" = NA)

fit_measures$X1f_6_items <- c(fitmeasures(mod_1f_6_items.fit)["rmsea"], 
                              fitmeasures(mod_1f_6_items.fit)["cfi"], 
                              fitmeasures(mod_1f_6_items.fit)["tli"], 
                              fitmeasures(mod_1f_6_items.fit)["agfi"])
fit_measures$X1f_6_items <- round(fit_measures$X1f_6_items,3)

mod_1f_5_items <- 'knowledge =~ gen_know_ep + gen_know_switzerland + mig_know_free_move + mig_know_schengen + mig_know_syrians'
mod_1f_5_items.fit <- cfa(mod_1f_5_items, data=reminder_data, ordered = TRUE)

fit_measures$X1f_5_items <- c(fitmeasures(mod_1f_5_items.fit)["rmsea"], 
                              fitmeasures(mod_1f_5_items.fit)["cfi"], 
                              fitmeasures(mod_1f_5_items.fit)["tli"], 
                              fitmeasures(mod_1f_5_items.fit)["agfi"])
fit_measures$X1f_5_items <- round(fit_measures$X1f_5_items,3)

mod_2f_6_items <- '
gen_knowledge =~ gen_know_ep + gen_know_party + gen_know_switzerland
mig_knowledge =~ mig_know_free_move + mig_know_schengen + mig_know_syrians'
mod_2f_6_items.fit <- cfa(mod_2f_6_items, data=reminder_data, ordered = TRUE)

fit_measures$X2f_6_items <- c(fitmeasures(mod_2f_6_items.fit)["rmsea"], 
                              fitmeasures(mod_2f_6_items.fit)["cfi"], 
                              fitmeasures(mod_2f_6_items.fit)["tli"], 
                              fitmeasures(mod_2f_6_items.fit)["agfi"])
fit_measures$X2f_6_items <- round(fit_measures$X2f_6_items,3)

mod_2f_5_items <- '
gen_knowledge =~ gen_know_ep + gen_know_switzerland
mig_knowledge =~ mig_know_free_move + mig_know_schengen + mig_know_syrians'
mod_2f_5_items.fit <- cfa(mod_2f_5_items, data=reminder_data, ordered = TRUE)

fit_measures$X2f_5_items <- c(fitmeasures(mod_2f_5_items.fit)["rmsea"], 
                              fitmeasures(mod_2f_5_items.fit)["cfi"], 
                              fitmeasures(mod_2f_5_items.fit)["tli"], 
                              fitmeasures(mod_2f_5_items.fit)["agfi"])
fit_measures$X2f_5_items <- round(fit_measures$X2f_5_items,3)


fit_measures$Measure <- c("RMSEA",
                          "CFI",
                          "TLI",
                          "AGFI")

fit_measures <- fit_measures %>% 
  rename("1D, 6 items" = X1f_6_items,
         "1D, 5 items" = X1f_5_items,
         "2D, 6 items" = X2f_6_items,
         "2D, 5 items" = X2f_5_items)

kable(fit_measures, 
      "latex", 
      booktabs = T,
      caption="CFA fit measures of General (G) and Immigration (I) Knowledge Questions for unidimensional (1D) and two-dimensional (2D) models", 
      label="tab3")
```

Reading across these two sets of analyses, the EFA results suggest that the knowledge questions are potentially tapping into distinctive dimensions -- which is not surprising, given that they were intended to measure different types of knowledge. At the same time, while the CFA results indicate that the two-dimensional models display superior fits, the fact that the unidimensional models also exhibit very good fits suggests that the generalist assumption is not obviously misguided at this stage.

## Step 2: Construct Validation

Factor analysis enables us to investigate the extent to which correlations between items can be plausibly explained with reference to a certain number of factors, or underlying traits. However, such analysis does not speak to whether the trait thereby measured is the intended one, i.e. to the matter of *construct validity*. For this reason, we turn attention to whether the correlations between each of the two knowledge scales and a variety of demographic variables appear the way we would expect if these scales were measuring forms of knowledge. Moreover, we examine whether the correlations exhibited by the two scales are similar, as they should be if the generalist assumption holds.

To do this, we first create two knowledge scales using IRT modeling [@deayala2009; demars2010]. IRT models measure latent traits assumed to fall on a continuous scale. Values on that scale are usually referred to by way of the Greek letter $\theta$ (theta), and taken to range from -$\infty$ to +$\infty$, with a mean of 0 and standard deviation of 1. This means that, while the individual $\theta$ value ascribed to any particular respondent has no intrinsic meaning, it can nevertheless be interpreted relative to an estimated population mean. Full details on the IRT models used in this section as well as those used subsequently can be found in the Supporting Information.

Having ensured that the two sets of items satisfy standard assumptions for an IRT model (unidimensionality, local independence, and acceptable model fit), we then regress these scales on gender, age, and education, across the entire data set (i.e., not segmenting by nationality). The first two variables are self-explanatory. In the data set, the education variable takes on a value from 0 to 8, corresponding to the nine International Standard Classification of Education (ISCED) levels of education, from early childhood education (0) to doctorate (8). Details about the resulting models can be found in Table \ref{tab:4}.

```{r tab4, echo=FALSE, results="asis"}
m_gen <- lm(know_score_general ~
              gender + # men know more
              age + # older know more
              education_ISCED, # more educated know more
            data = reminder_data)

m_imm <- lm(know_score_imm ~
              gender + # men know more
              age + # older know more
              education_ISCED, # more educated know more
            data = reminder_data)

texreg(list(m_gen, m_imm),
               single.row = TRUE,
               caption = "Regression models (OLS)",
               custom.model.names=c("General knowledge",
                                    "Immigration knowledge"),
               custom.coef.names = c("(Intercept)", "Male", "Age", "Education"),
               caption.above = TRUE,
               float.pos = "h!",
               custom.note="%stars.",
               stars = c(0.001, 0.01, 0.05),
               digits=3,
               label="tab:4")
```

Table \ref{tab:4} shows how the coefficient values in both cases cohere with what we know about the relationship between political knowledge and demographics: men tend to have more political knowledge than women [@vanheerdehudson2020; @plutzer2020], and the educated more political knowledge than the uneducated [@hebbelstrup2016]. This echoes the patterns found by @barabas2014 in their study of knowledge items. It also seems reasonable to assume that political knowledge increases with age, as indeed is the case on both models. Moreover, as can also be seen from the table, the associations are very similar between the two scales, which speaks in favor of unidimensionality: both scales appear to be measuring the same fundamental, underlying trait.

In terms of further construct validation, we also compare these two scales with other widely-used measures of political knowledge from the American National Elections Study (ANES), the Cooperative Election Study (CES), and the British Election Study (BES). All of the question batteries we report here form knowledge scales that satisfy standard IRT modeling assumptions (see Supporting Information for details). The ANES data set is the 2019 Pilot Study [@anes2019] (N = 3,000), which contains the following knowledge items:

> (ANES1) What job or political office is now held by John Roberts? (Correct: Chief Justice of the US Supreme Court)\
> (ANES2) What job or political office is now held by Angela Merkel? (Correct: Chancellor of Germany [in 2019])\
> (ANES3) For how many years is a United States Senator elected -- that is, how many years are there in one full term of office for a U.S. Senator? (Correct: 6 years)

The CES data set [@ansolabehere2021] (N = 61,000) contains the following knowledge items:

> Please indicate whether you've heard of this person and if so which party he or she is affiliated with (response options: Never heard of person; Republican; Democrat; Other Party/Independent; Not sure):\
> (CES1) *Name of Governor of respondent's state*.\
> (CES2) *Name of first US Senator of respondent's state*.\
> (CES3) *Name of second US Senator of respondent's state*.\
> (CES4) *Name of one member of the US House of Representatives of respondent's state*.

Finally, in the case of BES, we use Wave 17 of the 2014-2023 British Election Study Internet Panel [@fiedhouse2020] (N = 34,366) which contains the following knowledge items:

> (BES1) No-one may stand for parliament unless they pay a deposit (True)\
> (BES2) The Liberal Democrats favour a system of proportional representation (True)\
> (BES3) MPs from different parties are on parliamentary committees (True)\
> (BES4) The number of members of parliament is about 100 (False)

As the education variables differ between the REMINDER data set used above, and the BES, ANES, and CES data sets (the latter three use standard UK/US education categories rather than ISCED), we investigate the relationship between these knowledge scales and aforementioned demographic characteristics by plotting and comparing patterns of estimated marginal means, as per Figures \ref{fig:emmeans_plots1} and \ref{fig:emmeans_plots2}.

```{r preprocess_for_plot, echo=FALSE, message=FALSE, results=FALSE}
# function to calculate and plot marginal means
emmeans_function <- function(model, covariates, xlabs) {
  plot_list <- list()
  for (i in 1:length(covariates)) {
    emmeans_object <- summary(emmeans(model, specs = covariates[i]))
    p <- ggplot(emmeans_object) +
      aes_string(x = covariates[i], y = "emmean") +
      geom_line(group=1, color = cbPalette[1]) +
      geom_pointrange(aes(ymin=lower.CL, ymax=upper.CL), color = cbPalette[1]) +
      geom_hline(yintercept=0, linetype="dashed", alpha=0.5) +
      xlab(xlabs[i]) +
      ylab("Estimated marginal mean") +
      theme(text = element_text(size=global_font_size)) +
      theme_minimal()

    plot_list[[i]] <- p
  }
  return(ggarrange(plotlist = plot_list, nrow=1))
}

reminder_data <- read_csv("../../data/model_data_IRT.csv")
reminder_data <- reminder_data %>% 
  mutate(education_ISCED = factor(education_ISCED,
                                  ordered = T),
         age_cat = cut(age, breaks=c(18, 24, 34, 44, 54, 64, Inf), 
                       labels=c("18-24","25-34","35-45","45-54","55-64","65+"),
                       ordered_result = TRUE),
         gender = recode(gender,
                         "male" = "Male",
                         "female" = "Female"))

m_reminder_general <- lm(know_score_general ~
                           age_cat +
                           gender +
                           education_ISCED,
                         data = reminder_data)

m_reminder_imm <- lm(know_score_imm ~
                       age_cat +
                       gender +
                       education_ISCED,
                     data = reminder_data)

# benchmark data 1: bes wave 17
bes_data <- read_csv("../../data/bes_data.csv")
bes_data <- bes_data %>% 
  mutate(education = recode(education,
                            `0` = "None",
                            `1` = "<GCSE",
                            `2` = "GCSE",
                            `3` = "A-lvl.",
                            `4` = "UG",
                            `5` = "PG"),
         education = factor(education,
                            levels = c("None",
                                       "<GCSE",
                                       "GCSE",
                                       "A-lvl.",
                                       "UG",
                                       "PG"),
                            ordered = T),
         age = recode(age,
                      `1` = "18-24",
                      `2` = "25-34",
                      `3` = "35-44",
                      `4` = "45-54",
                      `5` = "55-64",
                      `6` = "65+",
                      `7` = "65+",
                      `8` = "65+"),
         gender = recode(gender,
                         `1` = "Male",
                         `2` = "Female"))

m_bes <- lm(ability ~
              age +
              gender +
              education,
            data = bes_data)

# benchmark data 2: anes 2019
anes_data <- read_csv("../../data/anes_data.csv")
anes_data <- anes_data %>% 
  mutate(educ = factor(educ,
                       levels = c("no_hs",
                                  "high_school",
                                  "some_college",
                                  "two_yr_college",
                                  "four_yr_college",
                                  "post_grad"),
                       ordered = T),
         educ = recode(educ,
                       "no_hs" = "None",
                       "high_school" = "HS",
                       "some_college" = "Some coll.",
                       "two_yr_college" = "2yr",
                       "four_yr_college" = "4yr",
                       "post_grad" = "PG"),
         gender = recode(gender,
                         `1` = "Male",
                         `0` = "Female"),
         age_cat = cut(age, breaks=c(18, 24, 34, 44, 54, 64, Inf), 
                       labels=c("18-24","25-34","35-45","45-54","55-64","65+"),
                       ordered_result = TRUE))

m_anes <- lm(ability ~
               age_cat +
               gender +
               educ,
             data = anes_data)

# benchmark data 3: cces 2020
cces_data <- read_csv("../../data/cces_data.csv")

cces_data <- cces_data %>% 
         mutate(educ = factor(educ,
                       levels = c("None",
                                  "HS",
                                  "Some coll.",
                                  "2yr",
                                  "4yr",
                                  "PG"),
                       ordered = T),
                age_cat = factor(age_cat, 
                          levels=c("18-24",
                                   "25-34",
                                   "35-45",
                                   "45-54",
                                   "55-64",
                                   "65+"),
                       ordered = T))

m_cces <- lm(knowledge ~
               age_cat +
               gender +
               educ,
             data = cces_data)

# covid data
covid_data <- read_csv("../../data/covid_data.csv")
covid_data <- covid_data %>% 
  mutate(education = factor(education, 
                            levels=c("No formal education",
                                     "GCSE (or equivalent)",
                                     "A-level (or equivalent)",
                                     "Undergraduate degree (e.g., BA)",
                                     "Postgraduate degree (e.g., MA, MSc, PhD)"),
                            ordered = T),
         education = recode(education,
                            "No formal education" = "None",
                            "GCSE (or equivalent)" = "GCSE",
                            "A-level (or equivalent)" = "A-level",
                            "Undergraduate degree (e.g., BA)" = "UG",
                            "Postgraduate degree (e.g., MA, MSc, PhD)" = "PG"),
         age_cat = cut(age, breaks=c(18, 24, 34, 44, 54, 64, Inf), 
                       labels=c("18-24","25-34","35-45","45-54","55-64","65+"),
                       ordered_result = TRUE))

m_covid <- lm(know ~
                age_cat +
                gender +
                education,
              data = covid_data)

# epcc data
epcc_data <- read_csv("../../data/epcc_data.csv")
epcc_data <- epcc_data %>% 
  filter(education != "other" & education != "student") %>% 
  mutate(education = factor(education, 
                            levels=c("no_qual","gcse","alevel","degree"),
                            ordered = T),
         education = recode(education,
                            "no_qual" = "None",
                            "gcse" = "GCSE",
                            "alevel" = "A-level",
                            "degree" = "Degree"),
         gender = recode(gender,
                         "male" = "Male",
                         "female" = "Female"),
         age = factor(age,
                      levels = c("15-24",
                                 "25-34",
                                 "35-44",
                                 "45-54",
                                 "55-64",
                                 "65+"),
                      ordered = T))

m_epcc <- lm(knowledge ~
               age +
               gender +
               education,
             data = epcc_data)
```

```{r emmeans_plots1, fig.cap = "Estimated marginal means for REMINDER knowledge scales", echo=FALSE, fig.width=11,fig.height=7}
p1 <- emmeans_function(m_reminder_general,
                 c("education_ISCED","gender","age_cat"),
                 c("Highest level of education (ISCED)","Gender","Age bracket"))

p2 <- emmeans_function(m_reminder_imm,
                 c("education_ISCED","gender","age_cat"),
                 c("Highest level of education (ISCED)","Gender","Age bracket"))

ggarrange(plotlist = list(annotate_figure(p1, top = text_grob("REMINDER: General knowledge", 
               color = "black", size = 16)),
                          annotate_figure(p2, top = text_grob("REMINDER: Immigration knowledge", 
               color = "black", size = 16))),
          nrow=2)
```

```{r emmeans_plots2, fig.cap = "Estimated marginal means for BES and ANES knowledge scales", echo=FALSE, fig.width=11,fig.height=10.5}

p3 <- emmeans_function(m_anes,
               c("educ","gender","age_cat"),
                 c("Highest level of education","Gender","Age bracket"))

p4 <- emmeans_function(m_cces,
                 c("educ","gender","age_cat"),
                 c("Highest level of education","Gender","Age bracket"))

p5 <- emmeans_function(m_bes,
                 c("education","gender","age"),
                 c("Highest level of education","Gender","Age bracket"))

ggarrange(plotlist = list(annotate_figure(p3, top = text_grob("ANES", 
               color = "black", size = 16)),
               annotate_figure(p4, top = text_grob("CES", 
               color = "black", size = 16)),
               annotate_figure(p5, top = text_grob("BES", 
               color = "black", size = 16))), 
          nrow=3)
```

Figure \ref{fig:emmeans_plots1} shows that the estimated marginal means for each of the three demographic variables are very similar across the general and migration-specific knowledge scales. This is consistent with the evidence from previous sections for the generalist assumption: if both scales are tapping into the same underlying trait, then we should indeed expect the marginal means to exhibit similar patterns across these variables, as indeed they do. 

Turning then to Figure \ref{fig:emmeans_plots2}, the fact that the two scales from the previous figure exhibit similar patterns in their marginal means to the three established scales from the questions contained within the ANES, CES, and BES offers further evidence for both the generalist assumption, and for construct validity, i.e. for the scales tapping into a particular form of knowledge.

Finally, as shown in Figures \ref{fig:emmeans_plots3a} and \ref{fig:emmeans_plots3b}, breaking down the estimated means from the REMINDER scales by country suggests that the observations made about the aggregated data appear to hold in the cases of individual countries, too. While the smaller sample sizes render the results somewhat more noisy, this offers further evidence that the generalist assumption is robust in the face of geographic segmentation.

```{r emmeans_plots3a, fig.cap = "Estimated marginal means for general knowledge scale by country", echo=FALSE, fig.width=11,fig.height=16}

means_by_nationality <- function(nationalities, knowledge_var) {
  plot_list <- list()
  for (i in 1:length(nationalities)) {
    m <- lm(glue::glue("{knowledge_var} ~ age_cat + gender + education_ISCED"),
            data = subset(reminder_data, nationality==nationalities[i]))
    plot <- emmeans_function(m,
                             c("education_ISCED","gender","age_cat"),
                             c("Highest level of education","Gender","Age bracket"))
    plot_list[[i]] <- annotate_figure(plot, top = text_grob(str_to_upper(nationalities[i], locale = "en"), color = "black", size = 12))
  }
  return(ggarrange(plotlist = plot_list, nrow=7))
}

means_by_nationality(c("germany","hungary","poland","romania","spain","sweden","uk"), "know_score_general")
```

```{r emmeans_plots3b, fig.cap = "Estimated marginal means for immigration knowledge scale by country", echo=FALSE, fig.width=11,fig.height=16}

means_by_nationality(c("germany","hungary","poland","romania","spain","sweden","uk"), "know_score_imm")
```

## Step 3: Extending the Generalist Assumption beyond Migration

So far, we have provided evidence for the generalist assumption in the context of general political knowledge versus migration-specific knowledge. However, that analysis cannot rule out that there is something unique about migration-specific knowledge, perhaps in it having an especially close connection to general political knowledge. Therefore, in our third step, we compare the marginal means of the two REMINDER knowledge scales to those of two, separate issue-specific scales: one relating to public health (specifically knowledge about COVID-19) and one relating to knowledge about climate change.

Our COVID-19 data set comes from a a pre-registered survey experiment (N = 2,917) fielded in the UK through the wake of its first pandemic wave in July 2020. The survey contains the following knowledge items, measured pre-treatment, which together form a scale satisfying standard assumptions for IRT (again, see the Supporting Information document for further details on the IRT models):

> (COV1) COVID-19 can be transmitted in areas with hot and humid climate (True)\
> (COV2) There is currently no vaccine to protect against COVID-19 (True [in July 2020])\
> (COV3) Most people who get COVID-19 recover from it (True)

The climate change data set is from the European Perceptions of Climate Change survey [@pidgeon2016]. The survey was not specifically designed to measure knowledge about climate change, but it does contain three items which could reasonably be taken to tap into such knowledge[^3], and moreover form a scale satisfying standard assumptions for IRT:

> (CC1) As far as you know, do you think the world's climate is changing or not? (Correct: The climate is changing)\
> (CC2) Thinking about the causes of climate change, which, if any, of the following best describes your opinion? (Correct: Climate change is partly, mainly, or completely caused by human activity)\
> (CC3) To the best of your knowledge, what proportion of scientists agree that climate change is happening and that humans are largely causing it? (Correct: The vast majority of scientists agree, at 80% or more)

[^3]: As (CC2) is formulated in terms of the respondent's opinion, it might be taken not to measure someone's level of knowledge. However, as the item asks about the respondent's opinion on a matter of fact, and any opinion taken by the respondent thereby can be deemed correct or incorrect, we treat it as a knowledge item.

These topics, and the questions within them, are useful for stress-testing the generality of our results: while they both deal with policy-specific domains, they vary in terms of temporal dimension [@barabas2014]. Facts about climate change have arguably become well-established in public consciousness, whereas possessing knowledge about COVID-19 -- at the time of the survey fieldwork -- would likely indicate a degree of active surveillance by respondents. Considering these differences matters because they mirror theoretical expectations of how knowledge informs political behavior. On the one hand, knowledge of fundamental and unchanging political rules potentially enables citizens to actively participate in politics [@carpinikeeter1996]. On the other hand, knowing more recent facts potentially signals an ability to learn about -- and have views on -- political events as they happen [@schudson2011]. 

To that end, Figure \ref{fig:emmeans_plots4} plots the estimated marginal means for the same demographic variables as in the previous section for the COVID and climate change knowledge scales.

```{r emmeans_plots4, fig.cap = "Estimated marginal means for COVID and Climate Change knowledge scales", echo=FALSE, fig.width=11,fig.height=7}
p6 <- emmeans_function(m_covid,
                 c("education","gender","age_cat"),
                 c("Education","Gender","Age bracket"))

p7 <- emmeans_function(m_epcc,
                 c("education","gender","age"),
                 c("Highest level of education","Gender","Age bracket"))

ggarrange(plotlist = list(annotate_figure(p6, top = text_grob("COVID knowledge", 
               color = "black", size = 16)),
                          annotate_figure(p7, top = text_grob("Climate change knowledge", 
               color = "black", size = 16))), 
          nrow=2)
```

For both scales, the marginal means by education shows the same, monotonic trend seen in Figure \ref{fig:emmeans_plots1} and \ref{fig:emmeans_plots2}. In the case of both scales, men also generally have more knowledge than women, which again aligns with what we found in regards to the scales in the previous section. In the case of age, we see a more pronounced drop from the first to the second bracket than what we saw for the previous four scales. Additionally, the climate change scale in particular exhibits a less straightforward relationship than the other scales, in having substantially higher levels of knowledge in the first bracket. It is worth reflecting on whether knowledge about climate change might have a different distribution given the potential for higher levels of awareness among younger compared to older people, in light of how the former have a greater number of years of exposure to the effects of climate change ahead of them.

# Discussion and Practical Guidance

A long line of research demonstrates how being more politically informed potentially matters for a host of outcomes, including what people think and prefer about issues. However, whether on principled or practical grounds, much of this work invokes what we have called the *generalist assumption*: that knowledge about general political topics is indicative of knowledge about issue-specific topics. From a measurement perspective, this assumption is useful because it allows researchers to use necessarily limited question batteries as proxies for other unmeasured forms of knowledge that are actually of interest. Yet despite its centrality to core public opinion work [@zaller1992], this assumption is difficult to empirically test owing to the lack of surveys that contain general and specific knowledge questions. Having evidence of whether and in which circumstances it holds would present implications for research that uses knowledge as an explanatory variable.

In response, we have exploited a recent, large-scale survey that, unusually, contains both types of items in one instrument across seven European countries. Combining several analytical approaches, we conclude that the preponderance of evidence points to the generalist assumption being defensible. First, exploratory and confirmatory factor analyses applied to various combinations of the knowledge items revealed how the collection of questions performed well when treated unidimensionally. Second, in terms of construct validity, key demographic features that prior work leads us to expect are relevant to political knowledge exhibit similar associations between our political and immigration knowledge scales. Moreover, both scales in the aggregate and by country exhibit similar patterns in estimated marginal mean level of knowledge to established knowledge scales built from questions in the ANES, CES, and BES. Third, in an extension to migration, we observe the same patterns in the case of issue-specific knowledge scales about COVID-19 and climate change which reflect knowledge based in both longer- and shorter-term events.

Our study has implications for the ways that researchers use political knowledge questions as they study domain-specific attitudes and preferences. Although we argue the generalist assumption is defensible, we also emphasize that it should not be deployed uncritically. Rather, we advise that researchers should stress-test it as far as available data sets allow, notably by including knowledge items from different domains, and by using a combination of methods to evaluate its plausibility in specific contexts. Having evidence of this would be useful in at least two situations: where researchers want to evaluate the dimensionality of knowledge items available to them to decide which scales to use; and where researchers are collecting data, want to run a pilot with a range of knowledge items, and then choose the minimum number of items needed to achieve their objectives to potentially save funds and survey time.

Therefore, we offer the following guidelines informed by our analysis:

1.  Start by conducting a parallel analysis on the total set of knowledge items, and then investigate the loadings from an exploratory factor analysis (EFA) on those same items, where the number of factors assumed in the EFA is informed by the parallel analysis.\
2.  Let the EFA results inform a confirmatory factor analysis (CFA) that compares a unidimensional model to one or several multidimensional models, depending on the number of factors suggested by the EFA, and in order to see whether the former exhibits a good enough fit to compete with the multidimensional model(s). If it can, this would point towards unidimensionality.\
3.  Construct two or more knowledge scales, depending on the dimensionality under investigation, and then use each scale as an outcome in a regression model, with gender, education, and age as predictors. See if there any similarities in coefficient values between the two (or more) models. High similarity across all predictors would count towards unidimensionality.\
4.  Estimate marginal mean levels of knowledge by gender, education, and age, respectively, for each of the scales, and compare the patterns in these levels to those exhibited in Figures \ref{fig:emmeans_plots1}, \ref{fig:emmeans_plots2} and \ref{fig:emmeans_plots4}. Similarity with the patterns found there would count towards both construct validity and unidimensionality.

For any given project, all or only some of these steps will be appropriate or possible. However, it is our hope that the above steps will nonetheless offer helpful guidance for researchers concerned with questions about knowledge scale dimensionality. To facilitate replication, and in line with Open Science principles, we also publicly offer the code underlying the present investigation as a template for others to use in a [public GitHub repository](https://github.com/ahlstromvij/REMINDER_project).

# References